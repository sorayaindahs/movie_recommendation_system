# -*- coding: utf-8 -*-
"""Another copy of REKOMENNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kHRk0DITIrvw_b7LQXQ1Q0KTAKaKjv3h

**SUBMISSION 1 - SISTEM REKOMENDASI**
- **Nama:** Soraya Indah Setiani
- **Email:** sorayaindahs58@gmail.com
- **ID Dicoding:** sorayasetiani

# **Import Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

"""# **Data Understanding**

## **Data Loading**
"""

# Dataset genre
urlgenre = 'https://drive.google.com/file/d/1ITF6El7MXZ1etHjrFZ5aXaSvf2YjmD01/view?usp=sharing'
genre = 'https://drive.google.com/uc?export=download&id='+urlgenre.split('/')[-2]
genre_df = pd.read_csv(genre, on_bad_lines='skip')
genre_df

genre_df.info()

# Dataset rating
urlrating = 'https://drive.google.com/file/d/1Dlh745aX5Tp1QmFHXyQ5Kb9L8g0hRUHQ/view?usp=sharing'
rating = 'https://drive.google.com/uc?export=download&id='+urlrating.split('/')[-2]
rating_df = pd.read_csv(rating, on_bad_lines='skip')
rating_df

rating_df.info()

"""## **Deskripsi Variabel**

Dataset genre terdiri dari **27278 baris** dan **3 kolom** dengan keterangan sebagai berikut.

- movieId: ID movie
- title: judul movie
- genres: Kategori movie

Setiap kolom memiliki jumlah baris yang sama, yaitu 27278.

Dataset rating terdiri dari **1048575 baris** dan **4 kolom** dengan keterangan sebagai berikut.

- userId: ID pengguna yang memberi rating
- movieId: ID movie
- rating: Penilaian dari user untuk movie
- timestamp: Waktu saat rating diberikan, dalam format waktu dan tanggal.

Setiap kolom memiliki jumlah baris yang sama, yaitu 1048575.

## **Univariate Exploratory Data Analysis**

### **Identifikasi Missing Value dan Outlier di Setiap Kolom**.

Tahap ini akan mengidentifikasi nilai yang hilang pada setiap kolom dan nilai-nilai yang tidak relevan.

####**Kolom movieId**
"""

# Jumlah unique movieId
genre_df['movieId'].nunique()

"""Terdapat 27278 movieId yang berarti platform menyediakan 27278 movie untuk pengguna."""

# Identifikasi missing value pada kolom movieID
genre_df['movieId'].isnull().sum()

"""Tidak terdapat missing value pada kolom movieId.

####**Kolom title**
"""

# Jumlah unique title
genre_df['title'].nunique()

# Identifikasi missing value pada kolom title
genre_df['title'].isnull().sum()

"""Tidak terdapat missing value pada kolom title.

Perbedaan jumlah movieId (27278) dan jumlah title (27262) bukan disebabkan oleh missing value kemungkinan besar karena adanya kesamaan judul movie dengan movieId berbeda. Bisa jadi karena perbedaan versi atau remake. Berikut identifikasi judul movie yang sama dengan movieId berbeda.
"""

# Hitung frekuensi kemunculan setiap title
title_counts = genre_df['title'].value_counts()

# Filter title yang muncul lebih dari satu kali
duplicate_titles = title_counts[title_counts > 1]

# Tampilkan
print(duplicate_titles)

"""Terdapat 16 movie dengan judul movie yang sama dan movieId berbeda. Hal ini masuk akal jika terdapat 27278 movideId dan 27262 title.

####**Kolom genres**
"""

# Identifikasi missing value pada kolom genre
genre_df['genres'].isnull().sum()

# identifikasi unique pada genres
all_genres = genre_df['genres'].str.split('|').sum()
unique_genres = set(all_genres)
print(sorted(unique_genres))

"""Terdapat 19 value genre dan 1 no genres listed"""

# Cek baris yang kolom genres berisi (no genres listed)
genre_df[genre_df['genres'] == '(no genres listed)']

"""Terdapat 246 movie yang tidak tertulis genre. Hal ini tidak perlu dilakukan penanganan seperti missing value karena akan menghapus banyak informasi. Meskipun akan sedikit mengganggu pada sistem rekomendasi content based, hal ini tidak terlalu bermasalah untuk penerapan collaborative filtering.

####**Kolom userId**
"""

# Identifikasi missing value pada kolom userId
rating_df['userId'].isnull().sum()

# Jumlah unique userId
rating_df['userId'].nunique()

# Jumlah unique userId
rating_df['userId'].unique()

"""Output kode menunjukkan bahwa tidak terdapat missing value pada kolom userId dan terdapat 7120 users yang memberi rating pada movie.

####**Kolom rating**
"""

# Identifikasi missing value pada kolom userId
rating_df['rating'].isnull().sum()

# Cek unique pada kolom rating
rating_df['rating'].unique()

"""Tidak terdapat missing value dan outlier pada kolom rating karena semua nilai berada pada rentang penilaian, yaitu 0.5 sampai 5.

####**Kolom timestamp**

Sebelum membangun model, perlu mengetahui kolom yang tidak akan digunakan dalam membangun sistem rekomendasi, seperti kolom timestamp. Kolom ini akan dihapus untuk mempermudah pemodelan. Penghapusan kolom timestamp akan dilakukan pada data preprocessing.

### **Identifikasi Duplikasi Data**
"""

# Cek duplikasi
genre_df.duplicated().sum()

"""Berdasarkan identifikasi, tidak terdapat duplikasi data yang perlu ditangani.

# **Data Preparation-Content Based Filtering**

Dataset yang digunakan dalam content based filtering adalah dataset genre saja tanpa digabung dengan rating. Sebelum membangun model, dataset genre_id yang telah siap dapat assign ke variabel data.
"""

data = genre_df
data.sample(5)

"""## **TF-IDF Vectorizer**

Model sistem rekomendasi sederhana akan dibangun berdasarkan genre dari movie. Teknik TF-IDF Vectorizer akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap genre movie. Sebelum itu, perlu menyatukan value genres yang lebih dari 1 kata, yaitu Sci-Fi dan no genres listed.
"""

data['genres'] = data['genres'].str.replace('Sci-Fi', 'Sci_Fi')
data['genres'] = data['genres'].str.replace('movie-Noir', 'movie_Noir')
data['genres'] = data['genres'].str.replace('no genres listed', 'no_genres_listed')

# Inisialisasi TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Melakukan perhitungan idf pada data genres
tfidf_vectorizer.fit(data['genres'])

# Mapping array dari fitur index integer ke fitur nama
tfidf_vectorizer.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(data['genres'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Matriks berukuran (27278, 20). Nilai 1048575 merupakan ukuran data dan 20 merupakan matriks genre."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Selanjutnya, melihat matriks tf-idf untuk beberapa judul movie (title) dan genre movie (genres)."""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan genre movie
# Baris diisi dengan judul movie

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf_vectorizer.get_feature_names_out(),
    index=data.title
).sample(20, axis=1).sample(10, axis=0)

"""Oleh karena beberapa movie tidak hanya memiliki 1 genre, tetapi beberapa genre yang relevan dengan movie tersebut, sehingga matrix tidak bernilai bulat 1.

## **Cosine Similarity**

Pada tahap sebelumnya, telah berhasil mengidentifikasi korelasi antara judul movie dengan genrenya. Selanjutnya, perlu menghitung derajat kesamaan (similarity degree) antar movie dengan teknik cosine similarity. Pembatasan data dilakukan agar proses dapat berjalan.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)

"""Berikut matriks kesamaan setiap movie  dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0)."""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul movie
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap movie
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""# **Model Development: Content Based Filtering**"""

def movie_recommendations(title, similarity_data=cosine_sim_df, items=None, k=5):
    """
    Rekomendasi Movie berdasarkan kemiripan dataframe

    Parameter:
    ---
    title : tipe data string (str)
                Nama Movie (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan title sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """

    if items is not None:
        items['genre_set'] = items['genres'].apply(lambda x: set(x.split('|')))
    else:
        print("Warning: 'items' DataFrame not provided. Cannot add 'genre_set' column.")


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index.flatten()[-1:-(k+2):-1]]

    # Drop title agar judul movie yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(title, errors='ignore')

    if isinstance(items, pd.DataFrame):
        return pd.DataFrame({'title': closest}).merge(items, on='title', how='left')[['title', 'genres']].head(k)
    else:
        return pd.DataFrame(closest).head(k)

"""Menggunakan argpartition, sejumlah nilai k tertinggi dapat diambil dari similarity data (dalam kasus ini: dataframe cosine_sim_df). Kemudian,  data diambil dari bobot (tingkat kesamaan) tertinggi ke terendah. Data ini dimasukkan ke dalam variabel closest. Berikutnya, title yang dicari perlu dihapus agar tidak muncul dalam daftar rekomendasi.

Penjelasan parameter yang digunakan pada model, antara lain:
- title: berfungsi jadi dasar pencarian untuk rekomendasi.
- similarity_data berisi default cosine_sim_df, ini akan digunakan untuk mencari movie mana yang paling mirip dengan title berdasarkan nilai cosine similarity.
- items: None, berarti judul hanya judul movie yang akan ditampilkan, tanpa informasi tambahan dari fitur lain.
- k = 5 menunjukkan 5 rekomendasi movie dengan kemiripan teratas.
"""

movie_recommendations('Toy Story (1995)', items=data[['title', 'genres']], k=5)

"""Sistem memberikan 5 rekomendasi movie yang memiliki kemiripan dengan movie referensi tersebut berdasarkan genre menggunakan pendekatan content-based filtering. Contoh di atas, sistem akan merekomendasikan kepada pengguna  5 movie dengan kemiripan genre dari preferensinya, yaitu Toy Story (1995).

Kelebihan content based learning berdasarkan cara kerja:
- personalisasi tinggi karena sistem memberikan rekomendasi berdasarkan preferensi unik setiap pengguna.
- Tidak perlu menunggu data dari pengguna lain yang berinteraksi.

Kelemahan content based learning berdasarkan cara kerja:
- Sistem cenderung merekomendasikan item yang sangat mirip, sehingga kurang bervariasi.
- Jika pengguna belum banyak berinteraksi, sistem sulit membangun profil preferensi.
- Tidak melihat apa yang populer atau disukai pengguna lain

# **Data Preparation - Collaborative Filtering**

## **Menghapus Kolom timestamp**

Kolom timestamp dihapus karena tidak akan digunakan dalam membangun model sistem rekomendasi.
"""

# Menghapus kolom timestamp
rating_df.drop(['timestamp'], axis=1, inplace=True)

"""## **Penggabungan Dataset**

Untuk membangun model collaborative filtering, perlu penggabungan dataset genre_df dan rating_df berdasarkan kolom movieId
"""

movie_df = pd.merge(rating_df, genre_df, on='movieId', how='inner')
movie_df

"""## **Encode fitur userId dan movieId**"""

rating = rating_df

"""Encoding merupakan proses menyandikan fitur user dan placeID ke dalam indeks integer."""

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = rating['userId'].unique().tolist()
print('list userId: ', user_ids)

# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userId: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = rating['movieId'].unique().tolist()

# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""Memetakan userId dan movieId ke dataframe yang berkaitan"""

# Mapping userID ke dataframe user
rating['user'] = rating['userId'].map(user_to_user_encoded)

# Mapping placeID ke dataframe  movie
rating['movie'] = rating['movieId'].map(movie_to_movie_encoded)

"""Cek beberapa hal dalam data, seperti jumlah user, jumlah movie, dan mengubah nilai rating menjadi float64."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah movie
num_movie = len(movie_encoded_to_movie)
print(num_movie)

# Mengubah rating menjadi nilai float
rating['rating'] = rating['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(rating['rating'])

# Nilai maksimal rating
max_rating = max(rating['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""## **Membagi Data untuk Training dan Validasi**"""

# Mengacak dataset
rating = rating.sample(frac=1, random_state=42)
rating

"""Selanjutnya, membagi data train dan validasi dengan komposisi 80:20. Sebelum itu, perlu memetakan (mapping) data user dan movie menjadi satu value terlebih dahulu. Lalu, membuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = rating[['user', 'movie']].values

# Membuat variabel y untuk membuat rating dari hasil
y = rating['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# **Model Development: Collaborative Filtering**

## **Proses Training**

Pada tahap training, model menghitung skor kecocokan antara pengguna dan movie dengan teknik embedding. Pertama, proses embedding terhadap data user dan movie. Selanjutnya, operasi perkalian dot product antara embedding user dan movie. Selain itu, dapat juga menambahkan bias untuk setiap user dan movie. Skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid.
"""

class RecommenderNet(tf.keras.Model):
  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = tf.keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""Selanjutnya, proses compile terhadap model."""

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer, dan root mean squared error (RMSE) sebagai metrics evaluation.
- num_users merupakan jumlah users. Ini digunakan untuk membangun embedding layer user.
- num_movie merupakan jumlah movie. Ini digunakan untuk membangun embedding layer movie.
- 50 berarti setiap user dan movie akan diwakili oleh vektor berdimensi 50 yang dilatih.
- learning_rate=0.0001: Laju pembelajaran yang kecil, sehingga model akan belajar secara perlahan agar lebih stabil.
- metrics = [tf.keras.metrics.RootMeanSquaredError()].
RMSE digunakan untuk mengukur rata-rata kesalahan prediksi.

Langkah selanjutnya adalah proses training
"""

# Inisialisasi optimizer dengan learning rate kecil
optimizer = Adam(learning_rate=0.0001)

# Kompilasi model dengan optimizer dan loss function
model.compile(
    optimizer=optimizer,
    loss='mean_squared_error',
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Mulai training
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=256,
    epochs=30,
    validation_data=(x_val, y_val),
)

"""- learning_rate=0.0001: Laju pembelajaran yang kecil, sehingga model akan belajar secara perlahan agar lebih stabil.
- loss='mean_squared_error' untuk menghitung error.
- metrics=[tf.keras.metrics.RootMeanSquaredError()]: metrik evaluasi, yaitu RMSE yang digunakan untuk mengevaluasi prediksi rating.
- x_train: Input data training.
- y_train: Label target
- batch_size=256 artinya 256 sampeliterasi yang diproses sebelum perubahan weight.
- epoch = 30 artinya perulangan penuh terhadap seluruh dataset pelatihan sebanyak 30 kali.

## **Mendapatkan Rekomendasi**

Untuk mendapatkan rekomendasi movie, sampel user awalnya acak dan definisikan variabel movie_not_visited yang merupakan daftar movie yang belum pernah dikunjungi oleh pengguna.

Sebelumnya, pengguna telah memberi rating pada beberapa movie yang telah ditonton. Rating ini digunakan untuk membuat rekomendasi movie lain yang mungkin cocok dan belum pernah ditonton untuk pengguna.
"""

movie_data = movie_df.drop_duplicates(subset='movieId')  # Data daftar movie
rating_data = rating_df       # Data rating user (dari ratings.csv)

# Pilih 1 user secara acak
user_id = rating_data.userId.sample(1).iloc[0]

# Ambil movie yang pernah dirating user tersebut
movie_visited_by_user = rating_data[rating_data.userId == user_id]

# Cari movie yang belum ditonton user
movie_not_visited = movie_data[~movie_data['movieId'].isin(movie_visited_by_user.movieId.values)]['movieId']

# Filter hanya yang ada di dictionary encoding
movie_not_visited = list(
    set(movie_not_visited).intersection(set(movie_to_movie_encoded.keys()))
)

# Ubah movie_id ke dalam bentuk encoded dan buat pasangan user+movie
movie_not_visited = [[movie_to_movie_encoded.get(x)] for x in movie_not_visited]
user_encoded = user_to_user_encoded.get(user_id)

# Gabungkan user ID dan movie ID jadi input array ke model
user_movie_array = np.hstack(
    ([[user_encoded]] * len(movie_not_visited), movie_not_visited)
)

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_data_rows = movie_data[movie_data['movieId'].isin(top_movie_user)]
for row in movie_data_rows.itertuples():
    print(row.title, ':', row.genres)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = movie_data[movie_data['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)

"""Penjelasan singkat terkait parameter dan fungsi yang digunakan:
- user_movie_array: array berisi pasangan [user_id, movie_id] yang akan dinilai oleh model.
- model.predict(...): menghasilkan prediksi rating untuk setiap pasangan tersebut.
- .flatten(): mengubah hasil prediksi dari bentuk 2D
- top_ratings_indices = ratings.argsort()[-10:][::-1] artiny mengambil 10 indeks tertinggi dari ratings (rekomendasi teratas)
- argsort(): Mengurutkan indeks dari rating terkecil ke terbesar.
- [::-1]: Membalik hasil agar urutan dari tertinggi ke terendah.
- movie_not_visited: daftar movie yang belum pernah ditonton oleh user.
- movie_encoded_to_movie: dictionary untuk mengonversi movie_encoded_id ke movieId asli.

Penjelasan hasil rekomendasi:

Showing recommendations for users: 3057
- User ini adalah pengguna aktif dalam sistem rekomendasi.
- Sistem melihat pengguna-pengguna lain yang memberikan rating tinggi untuk movie yang sama seperti pengguna 3057.
- Rekomendasi dilakukan berdasarkan pola historis rating dan prediksi model terhadap movie yang belum ditonton.

Kelebihan Collaborative filtering berdasarkan cara kerja:
- Tidak bergantung atribut item, seperti genre.
- Lebih variatif karena memberikan rekomendasi berdasarkan pola perilaku kolektif pengguna, bukan hanya berdasarkan kesamaan konten.
- Kualitas rekomendasi meningkat seiring bertambahnya data interaksi pengguna.

Kelemahan Collaborative filtering berdasarkan cara kerja:
- Untuk item barusulit direkomendasikan karena belum ada yang memberi rating atau interaksi.
- Bisa dimanipulasi melalui fake ratings atau spam users, sehingga sering tidak akurat.

# **Evaluasi**

## **Evaluasi Model Content Based Filtering: precision@k**
"""

def precision_at_k(recommended, ground_truth, k=5):
    recommended_top_k = recommended[:k]
    relevant_items = set(recommended_top_k).intersection(set(ground_truth))
    return len(relevant_items) / k

# Normalisasi
recommended_titles = [t.lower().strip() for t in movie_recommendations(
    'Toy Story (1995)',
    similarity_data=cosine_sim_df,
    items=data[['title', 'genres']],
    k=5
)['title'].tolist()]

ground_truth = [t.lower().strip() for t in [
    'Wild, The (2006)',
    'Antz (1998)',
    'The Magic Crystal (2011)',
    "Emperor's New Groove, The (2000)",
    'Toy Story Toons: Small Fry (2011)'
]]

# Precision@5
precision_score = precision_at_k(recommended_titles, ground_truth, k=5)
print(f'Precision@5: {precision_score:.2f}')

"""Berikut penjelasan terkait parameter yang digunakan.
- recommended: daftar judul film yang direkomendasikan oleh model.
- ground_truth: daftar film yang dianggap sebagai jawaban yang relevan.
- k: jumlah item teratas yang ingin dievaluasi, misal k=5 (Precision@5).
- recommended_top_k = recommended[:k]: Mengambil hanya K film teratas dari hasil rekomendasi.

precision@5: 1.00 (atau 100%) menunjukkan bahwa kelima movie memang direkomendasikan karena sesuai dengan preferensi pengguna berdasarkan genre.

## **Evaluasi Model Collaborative Filtering: RMSE**
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Identifikasi RMSE**

Training RMSE (biru)
- Epoch 1: RMSE = 0.2766
- Epoch 15: RMSE = 0.1968
- Epoch 30: RMSE = 0.1912

Hal ini menunjukkan bahwa nilai RMSE data training turun secara konsisten seiring bertambahnya epoch. Artinya, model berhasil belajar dari data training secara bertahap.

Validation RMSE (oranye)

- Epoch 1: val_RMSE = 0.2282
- Epoch 15: val_RMSE = 0.1982
- Epoch 30: val_RMSE = 0.1936

RMSE pada data validasi juga turun stabil dan selisihnya kecil terhadap RMSE training. Menunjukkan generalization model sangat baik (tidak overfitting)

## **Kaitan dengan problem statements**

Berikut evaluasi dari pembuatan model sistem rekomendasi movie jika dikaitkan dengan problem statements.

*Bagaimana sistem rekomendasi dapat membantu pengguna dalam menemukan movie yang sesuai dengan preferensi mereka secara efisien?*

- Sistem rekomendasi movie telah dibangun menggunakan pendekatan content-based dan collaborative filtering. Model ini akan membantu pengguna menemukan movie berdasarkan preferensi secara cepat dan akurat.

*Sistem rekomendasi apa yang paling efektif dalam menghasilkan rekomendasi movie?*
- Untuk pengguna dengan minat yang spesifik dan stabil, content based filtering cocok diimplementasikan karena mampu memberikan rekomendasi secara personal tanpa dipengaruhi pengguna lain.

- Untuk pengguna yang menyukai movie berdasarkan tren, collaborative filtering cocok diimplementasikan karena model ini dipengaruhi dengan interaksi pengguna lain, misalnya movie dengan rating tertinggi akan direkomendasikan.

*Bagaimana pengaruh penerapan sistem rekomendasi terhadap peningkatan durasi penggunaan platform penyedia movie oleh pengguna?*

- Implementasi model sistem rekomendasi ini akan membantu pengguna menghemat waktu dan biaya dalam mencari movie yang sesuai minat. Menurut sisi bisnis, platform akan lebih sering digunakan apabila pengguna merasa cocok dengan rekomendasi yang diberikan sesuai dengan preferensinya. Hal ini tentu saja meningkatkan durasi dan penilaian pengguna dalam menggunakan platform penyedia movie.
"""